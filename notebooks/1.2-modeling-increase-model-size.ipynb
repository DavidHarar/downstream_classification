{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In the previous experiment we tried multiple things but eventualy none of the networds were able to discriminate adequetly between classes. In a previous attempt we used Incemption model with 760K params and it got the best results so far. In this notebook we try to increase the model size. \n",
    "We also add some callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>reading</th>\n",
       "      <th>dates</th>\n",
       "      <th>one_year_until_death</th>\n",
       "      <th>CVA</th>\n",
       "      <th>first_AF_in_home_ECG</th>\n",
       "      <th>first_AF_in_Dispatch</th>\n",
       "      <th>AF</th>\n",
       "      <th>IHD</th>\n",
       "      <th>sp_MI_all</th>\n",
       "      <th>sp_CABG</th>\n",
       "      <th>CHF</th>\n",
       "      <th>sp_CPR</th>\n",
       "      <th>cpr_shl</th>\n",
       "      <th>DM2</th>\n",
       "      <th>AF2</th>\n",
       "      <th>DM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>427094</td>\n",
       "      <td>01/09/2008</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>449383</td>\n",
       "      <td>25/05/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>511490</td>\n",
       "      <td>19/08/2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>419384</td>\n",
       "      <td>04/05/2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train</td>\n",
       "      <td>451966</td>\n",
       "      <td>17/07/2005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629979</th>\n",
       "      <td>train</td>\n",
       "      <td>336872</td>\n",
       "      <td>27/02/2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629980</th>\n",
       "      <td>train</td>\n",
       "      <td>281503</td>\n",
       "      <td>16/01/2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629981</th>\n",
       "      <td>train</td>\n",
       "      <td>275021</td>\n",
       "      <td>10/02/2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629982</th>\n",
       "      <td>train</td>\n",
       "      <td>37783</td>\n",
       "      <td>24/03/2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629984</th>\n",
       "      <td>train</td>\n",
       "      <td>443053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518366 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample  reading       dates  one_year_until_death  CVA  \\\n",
       "0       train   427094  01/09/2008                     1  0.0   \n",
       "1       train   449383  25/05/2018                     1  0.0   \n",
       "4       train   511490  19/08/2018                     0  0.0   \n",
       "5       train   419384  04/05/2014                     1  0.0   \n",
       "6       train   451966  17/07/2005                     1  0.0   \n",
       "...       ...      ...         ...                   ...  ...   \n",
       "629979  train   336872  27/02/2011                     1  0.0   \n",
       "629980  train   281503  16/01/2015                     1  0.0   \n",
       "629981  train   275021  10/02/2012                     0  0.0   \n",
       "629982  train    37783  24/03/2016                     1  0.0   \n",
       "629984  train   443053         NaN                     0  0.0   \n",
       "\n",
       "        first_AF_in_home_ECG  first_AF_in_Dispatch   AF  IHD  sp_MI_all  \\\n",
       "0                        1.0                   0.0  1.0  1.0        1.0   \n",
       "1                        0.0                   0.0  0.0  1.0        0.0   \n",
       "4                        1.0                   1.0  0.0  0.0        0.0   \n",
       "5                        1.0                   1.0  1.0  0.0        0.0   \n",
       "6                        1.0                   1.0  0.0  1.0        1.0   \n",
       "...                      ...                   ...  ...  ...        ...   \n",
       "629979                   1.0                   1.0  1.0  0.0        0.0   \n",
       "629980                   1.0                   1.0  1.0  1.0        0.0   \n",
       "629981                   1.0                   1.0  0.0  1.0        0.0   \n",
       "629982                   1.0                   1.0  0.0  0.0        0.0   \n",
       "629984                   0.0                   0.0  0.0  0.0        0.0   \n",
       "\n",
       "        sp_CABG  CHF  sp_CPR  cpr_shl  DM2  AF2   DM  \n",
       "0           0.0  1.0     1.0      0.0  1.0  1.0  1.0  \n",
       "1           0.0  0.0     0.0      0.0  1.0  0.0  1.0  \n",
       "4           0.0  0.0     0.0      0.0  0.0  0.0  0.0  \n",
       "5           0.0  0.0     0.0      0.0  0.0  1.0  0.0  \n",
       "6           0.0  0.0     0.0      0.0  0.0  0.0  0.0  \n",
       "...         ...  ...     ...      ...  ...  ...  ...  \n",
       "629979      0.0  0.0     0.0      0.0  0.0  1.0  0.0  \n",
       "629980      1.0  0.0     0.0      0.0  1.0  1.0  1.0  \n",
       "629981      0.0  0.0     0.0      0.0  0.0  0.0  0.0  \n",
       "629982      0.0  0.0     0.0      0.0  0.0  0.0  0.0  \n",
       "629984      0.0  0.0     0.0      0.0  0.0  0.0  0.0  \n",
       "\n",
       "[518366 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "_ = pd.read_csv('./data/combined_data/metadata_balanced_by_death.csv', index_col=0)\n",
    "_ [_['sample'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # general\n",
    "    'seed': 123,\n",
    "    'metadata_file_path': './data/combined_data/metadata_balanced_by_death.csv',\n",
    "    'data_folder_path': './data/individual-signals/',\n",
    "    'fillna': 0,\n",
    "    # training\n",
    "    'batch_size': 32,\n",
    "    'n_epochs': 30,\n",
    "    # architecture\n",
    "    'targets': ['one_year_until_death'],\n",
    "    'input_dimension': 12,\n",
    "    'hidden_dimmension':  64 ,          # d_model (int) â€“ the number of expected features in the input (required)???\n",
    "    'attention_heads': 4 ,            # number of attention heads, if None then d_model//64\n",
    "    'encoder_number_of_layers': 8,\n",
    "    'dropout': 0.4,\n",
    "    'clip': 1,\n",
    "    'positional_encodings': False,\n",
    "    'saving_path': './models/exp1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training using device: mps\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/transformer.py:218: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  self.layers[0].self_attn.batch_first was not True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 4\n",
      "The model has 683,016 trainable parameters\n",
      "TransformerEncoderDownstream(\n",
      "  (project_input): Linear(in_features=12, out_features=64, bias=True)\n",
      "  (encoder): Linear(in_features=12, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.4, inplace=False)\n",
      "        (dropout2): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=900, out_features=1, bias=True)\n",
      "  (dropout1): Dropout(p=0.4, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (conv1d): Conv1d(64, 2, kernel_size=(128,), stride=(1,))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Within epoch loss (training) 0.69344: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16198/16198 [4:20:55<00:00,  1.03it/s]  \n",
      "Within epoch loss (validation) 0.68617: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1680/1680 [07:35<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 examples:\n",
      "[0. 0. 0. 0. 0.] [0.49634621 0.49634621 0.49634621 0.49634621 0.49634621]\n",
      "Lengths 53760 53760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Epoch: 01 | Time: 268m 31s\n",
      "\tTrain Loss: 0.693\n",
      "\t Val. Loss: 0.686\n",
      "\t Best Val. Loss: 0.686\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Within epoch loss (training) 0.69318:  21%|â–ˆâ–ˆ        | 3435/16198 [52:36<3:15:28,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39mchdir(\u001b[39m'\u001b[39m\u001b[39m/Users/davidharar/Documents/School/thesis/downstream_classification\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexecutors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrain_on_local_machine_mps\u001b[39;00m \u001b[39mimport\u001b[39;00m trainer\n\u001b[0;32m----> 5\u001b[0m trainer(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Documents/School/thesis/downstream_classification/executors/train_on_local_machine_mps.py:112\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(seed, metadata_file_path, data_folder_path, fillna, targets, input_dimension, hidden_dimmension, attention_heads, encoder_number_of_layers, positional_encodings, dropout, clip, batch_size, n_epochs, saving_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    110\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# number of leads (dictates training scheme):\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m train_loss \u001b[39m=\u001b[39m train_epoch(model, \n\u001b[1;32m    113\u001b[0m                    train_generator, \n\u001b[1;32m    114\u001b[0m                    optimizer, \n\u001b[1;32m    115\u001b[0m                    criterion, \n\u001b[1;32m    116\u001b[0m                    clip, \n\u001b[1;32m    117\u001b[0m                    device \n\u001b[1;32m    118\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m \u001b[39m# evaluate\u001b[39;00m\n\u001b[1;32m    122\u001b[0m valid_loss, y_val, y_val_pred \u001b[39m=\u001b[39m evaluate_epoch(model, \n\u001b[1;32m    123\u001b[0m                       validation_generator, \n\u001b[1;32m    124\u001b[0m                       criterion, \n\u001b[1;32m    125\u001b[0m                       device,\n\u001b[1;32m    126\u001b[0m                       output_dimension\n\u001b[1;32m    127\u001b[0m                       )\n",
      "File \u001b[0;32m~/Documents/School/thesis/downstream_classification/executors/train_on_local_machine_mps.py:216\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, iterator, optimizer, criterion, clip, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    215\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), clip)\n\u001b[0;32m--> 216\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    217\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    219\u001b[0m j \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(epoch_loss\u001b[39m/\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/optimizer.py:316\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    319\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/optimizer.py:51\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     50\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 51\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/adamw.py:174\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     amsgrad \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    172\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_group(\n\u001b[1;32m    175\u001b[0m         group,\n\u001b[1;32m    176\u001b[0m         params_with_grad,\n\u001b[1;32m    177\u001b[0m         grads,\n\u001b[1;32m    178\u001b[0m         amsgrad,\n\u001b[1;32m    179\u001b[0m         exp_avgs,\n\u001b[1;32m    180\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    181\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    182\u001b[0m         state_steps,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m     adamw(\n\u001b[1;32m    186\u001b[0m         params_with_grad,\n\u001b[1;32m    187\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/torch/optim/adamw.py:109\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init_group\u001b[39m(\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     99\u001b[0m     group,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     state_steps,\n\u001b[1;32m    107\u001b[0m ):\n\u001b[1;32m    108\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 109\u001b[0m         \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39;49mgrad \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    111\u001b[0m         params_with_grad\u001b[39m.\u001b[39mappend(p)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# make sure we're in the correct directory\n",
    "os.chdir('/Users/davidharar/Documents/School/thesis/downstream_classification')\n",
    "from executors.train_on_local_machine_mps import trainer\n",
    "trainer(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

2023-12-27 18:57 - INFO - Fit the preprocessing pipeline
2023-12-27 18:57 - INFO - Training using device: cuda
2023-12-27 18:57 - INFO - Creating generators
2023-12-27 18:57 - INFO - The model has 96,498,539 trainable parameters
2023-12-27 18:57 - INFO - * Model:
2023-12-27 18:57 - INFO - * -----------
2023-12-27 18:57 - INFO - DownstreamInceptionResnet(
  (layers): ModuleDict(
    (input): DownstreamInceptionResnetBlock(
      (conv1): ConvBlock(
        (conv): Conv1d(12, 64, kernel_size=(7,), stride=(2,), padding=(3,))
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv2): ConvBlock(
        (conv): Conv1d(64, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (inception3a): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(256, 96, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(96, 256, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(256, 16, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(16, 64, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception3b): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(128, 384, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(512, 32, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(32, 192, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception4a): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(960, 384, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(960, 96, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(96, 416, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(960, 16, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(16, 96, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(960, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception4b): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(1024, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(1024, 112, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(112, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(1024, 24, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(24, 64, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(1024, 32, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (avgpool): AvgPool1d(kernel_size=(5,), stride=(1,), padding=(0,))
      (fc_resnet): Linear(in_features=8480, out_features=5400, bias=True)
      (fc_out): Linear(in_features=5400, out_features=1, bias=True)
      (relu): ReLU()
    )
    (hidden_1): DownstreamInceptionResnetBlock(
      (conv1): ConvBlock(
        (conv): Conv1d(12, 64, kernel_size=(7,), stride=(2,), padding=(3,))
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv2): ConvBlock(
        (conv): Conv1d(64, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (inception3a): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(256, 96, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(96, 256, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(256, 16, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(16, 64, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception3b): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(128, 384, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(512, 32, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(32, 192, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception4a): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(960, 384, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(960, 96, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(96, 416, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(960, 16, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(16, 96, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(960, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception4b): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(1024, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(1024, 112, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(112, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(1024, 24, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(24, 64, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(1024, 32, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (avgpool): AvgPool1d(kernel_size=(5,), stride=(1,), padding=(0,))
      (fc_resnet): Linear(in_features=8480, out_features=5400, bias=True)
      (fc_out): Linear(in_features=5400, out_features=1, bias=True)
      (relu): ReLU()
    )
    (output): DownstreamInceptionResnetFinal(
      (conv1): ConvBlock(
        (conv): Conv1d(12, 64, kernel_size=(7,), stride=(2,), padding=(3,))
        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv2): ConvBlock(
        (conv): Conv1d(64, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (inception3a): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(256, 96, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(96, 256, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(256, 16, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(16, 64, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception3b): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(128, 384, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(512, 32, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(32, 192, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception4a): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(960, 384, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(960, 96, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(96, 416, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(960, 16, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(16, 96, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(960, 128, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (inception4b): InceptionBlock(
        (branch1): ConvBlock(
          (conv): Conv1d(1024, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (branch2): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(1024, 112, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(112, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch3): Sequential(
          (0): ConvBlock(
            (conv): Conv1d(1024, 24, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): ConvBlock(
            (conv): Conv1d(24, 64, kernel_size=(5,), stride=(1,), padding=(2,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (branch4): Sequential(
          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (1): ConvBlock(
            (conv): Conv1d(1024, 32, kernel_size=(1,), stride=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (avgpool): AvgPool1d(kernel_size=(5,), stride=(1,), padding=(0,))
      (dropout): Dropout(p=0.1, inplace=False)
      (fc_out): Linear(in_features=8480, out_features=1, bias=True)
      (sigmoid): Sigmoid()
    )
  )
)
2023-12-27 18:57 - INFO - * -----------
2023-12-27 18:57 - INFO - Evaluating model based on: rocauc
2023-12-27 18:57 - INFO - Training..

2023-12-27 19:08 - INFO - ---------------------------------------------
2023-12-27 19:08 - INFO - Epoch: 01 | Time: 10m 39s
2023-12-27 19:08 - INFO - 	 New best val_rocauc loss was found, current best value is 0.66294
2023-12-27 19:08 - INFO - 	 Train Loss: 0.557
2023-12-27 19:08 - INFO - 	 Val. Loss: 0.524
2023-12-27 19:08 - INFO - 	 ROC-AUC: 0.663
2023-12-27 19:08 - INFO - 	 PR-AUC: 0.391
2023-12-27 19:08 - INFO - 	 Recall for 0.4 precision: 0.432
2023-12-27 19:08 - INFO - 	 Best Val. Loss: 0.524
2023-12-27 19:08 - INFO - 	 Best ROC-AUC: 0.663
2023-12-27 19:08 - INFO - 	 Best PR-AUC: 0.391
2023-12-27 19:08 - INFO - 	 Best Recall for 0.4 precision: 0.432
2023-12-27 19:08 - INFO - ---------------------------------------------
2023-12-27 19:19 - INFO - ---------------------------------------------
2023-12-27 19:19 - INFO - Epoch: 02 | Time: 11m 10s
2023-12-27 19:19 - INFO - 	 New best val_rocauc loss was found, current best value is 0.70636
2023-12-27 19:19 - INFO - 	 Train Loss: 0.516
2023-12-27 19:19 - INFO - 	 Val. Loss: 0.507
2023-12-27 19:19 - INFO - 	 ROC-AUC: 0.706
2023-12-27 19:19 - INFO - 	 PR-AUC: 0.430
2023-12-27 19:19 - INFO - 	 Recall for 0.4 precision: 0.577
2023-12-27 19:19 - INFO - 	 Best Val. Loss: 0.507
2023-12-27 19:19 - INFO - 	 Best ROC-AUC: 0.706
2023-12-27 19:19 - INFO - 	 Best PR-AUC: 0.430
2023-12-27 19:19 - INFO - 	 Best Recall for 0.4 precision: 0.577
2023-12-27 19:19 - INFO - ---------------------------------------------
2023-12-27 19:26 - INFO - ---------------------------------------------
2023-12-27 19:26 - INFO - Epoch: 03 | Time: 6m 44s
2023-12-27 19:26 - INFO - 	 New best val_rocauc loss was found, current best value is 0.7095
2023-12-27 19:26 - INFO - 	 Train Loss: 0.506
2023-12-27 19:26 - INFO - 	 Val. Loss: 0.506
2023-12-27 19:26 - INFO - 	 ROC-AUC: 0.709
2023-12-27 19:26 - INFO - 	 PR-AUC: 0.434
2023-12-27 19:26 - INFO - 	 Recall for 0.4 precision: 0.583
2023-12-27 19:26 - INFO - 	 Best Val. Loss: 0.506
2023-12-27 19:26 - INFO - 	 Best ROC-AUC: 0.709
2023-12-27 19:26 - INFO - 	 Best PR-AUC: 0.434
2023-12-27 19:26 - INFO - 	 Best Recall for 0.4 precision: 0.583
2023-12-27 19:26 - INFO - ---------------------------------------------
2023-12-27 19:33 - INFO - ---------------------------------------------
2023-12-27 19:33 - INFO - Epoch: 04 | Time: 7m 4s
2023-12-27 19:33 - INFO - 	 New best val_rocauc loss was found, current best value is 0.71864
2023-12-27 19:33 - INFO - 	 Train Loss: 0.502
2023-12-27 19:33 - INFO - 	 Val. Loss: 0.500
2023-12-27 19:33 - INFO - 	 ROC-AUC: 0.719
2023-12-27 19:33 - INFO - 	 PR-AUC: 0.445
2023-12-27 19:33 - INFO - 	 Recall for 0.4 precision: 0.614
2023-12-27 19:33 - INFO - 	 Best Val. Loss: 0.500
2023-12-27 19:33 - INFO - 	 Best ROC-AUC: 0.719
2023-12-27 19:33 - INFO - 	 Best PR-AUC: 0.445
2023-12-27 19:33 - INFO - 	 Best Recall for 0.4 precision: 0.614
2023-12-27 19:33 - INFO - ---------------------------------------------
2023-12-27 19:43 - INFO - ---------------------------------------------
2023-12-27 19:43 - INFO - Epoch: 05 | Time: 10m 8s
2023-12-27 19:43 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72037
2023-12-27 19:43 - INFO - 	 Train Loss: 0.500
2023-12-27 19:43 - INFO - 	 Val. Loss: 0.497
2023-12-27 19:43 - INFO - 	 ROC-AUC: 0.720
2023-12-27 19:43 - INFO - 	 PR-AUC: 0.446
2023-12-27 19:43 - INFO - 	 Recall for 0.4 precision: 0.618
2023-12-27 19:43 - INFO - 	 Best Val. Loss: 0.497
2023-12-27 19:43 - INFO - 	 Best ROC-AUC: 0.720
2023-12-27 19:43 - INFO - 	 Best PR-AUC: 0.446
2023-12-27 19:43 - INFO - 	 Best Recall for 0.4 precision: 0.618
2023-12-27 19:43 - INFO - ---------------------------------------------
2023-12-27 19:51 - INFO - ---------------------------------------------
2023-12-27 19:51 - INFO - Epoch: 06 | Time: 7m 58s
2023-12-27 19:51 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72037
2023-12-27 19:51 - INFO - 	 Train Loss: 0.499
2023-12-27 19:51 - INFO - 	 Val. Loss: 0.499
2023-12-27 19:51 - INFO - 	 ROC-AUC: 0.718
2023-12-27 19:51 - INFO - 	 PR-AUC: 0.444
2023-12-27 19:51 - INFO - 	 Recall for 0.4 precision: 0.605
2023-12-27 19:51 - INFO - 	 Best Val. Loss: 0.497
2023-12-27 19:51 - INFO - 	 Best ROC-AUC: 0.720
2023-12-27 19:51 - INFO - 	 Best PR-AUC: 0.446
2023-12-27 19:51 - INFO - 	 Best Recall for 0.4 precision: 0.618
2023-12-27 19:51 - INFO - ---------------------------------------------
2023-12-27 19:58 - INFO - ---------------------------------------------
2023-12-27 19:58 - INFO - Epoch: 07 | Time: 7m 11s
2023-12-27 19:58 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72124
2023-12-27 19:58 - INFO - 	 Train Loss: 0.499
2023-12-27 19:58 - INFO - 	 Val. Loss: 0.503
2023-12-27 19:58 - INFO - 	 ROC-AUC: 0.721
2023-12-27 19:58 - INFO - 	 PR-AUC: 0.450
2023-12-27 19:58 - INFO - 	 Recall for 0.4 precision: 0.623
2023-12-27 19:58 - INFO - 	 Best Val. Loss: 0.497
2023-12-27 19:58 - INFO - 	 Best ROC-AUC: 0.721
2023-12-27 19:58 - INFO - 	 Best PR-AUC: 0.450
2023-12-27 19:58 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 19:58 - INFO - ---------------------------------------------
2023-12-27 20:08 - INFO - ---------------------------------------------
2023-12-27 20:08 - INFO - Epoch: 08 | Time: 9m 32s
2023-12-27 20:08 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 20:08 - INFO - 	 Train Loss: 0.499
2023-12-27 20:08 - INFO - 	 Val. Loss: 0.496
2023-12-27 20:08 - INFO - 	 ROC-AUC: 0.723
2023-12-27 20:08 - INFO - 	 PR-AUC: 0.450
2023-12-27 20:08 - INFO - 	 Recall for 0.4 precision: 0.620
2023-12-27 20:08 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 20:08 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 20:08 - INFO - 	 Best PR-AUC: 0.450
2023-12-27 20:08 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 20:08 - INFO - ---------------------------------------------
2023-12-27 20:19 - INFO - ---------------------------------------------
2023-12-27 20:19 - INFO - Epoch: 09 | Time: 11m 2s
2023-12-27 20:19 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 20:19 - INFO - 	 Train Loss: 0.498
2023-12-27 20:19 - INFO - 	 Val. Loss: 0.509
2023-12-27 20:19 - INFO - 	 ROC-AUC: 0.705
2023-12-27 20:19 - INFO - 	 PR-AUC: 0.429
2023-12-27 20:19 - INFO - 	 Recall for 0.4 precision: 0.555
2023-12-27 20:19 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 20:19 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 20:19 - INFO - 	 Best PR-AUC: 0.450
2023-12-27 20:19 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 20:19 - INFO - ---------------------------------------------
2023-12-27 20:30 - INFO - ---------------------------------------------
2023-12-27 20:30 - INFO - Epoch: 10 | Time: 10m 58s
2023-12-27 20:30 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 20:30 - INFO - 	 Train Loss: 0.498
2023-12-27 20:30 - INFO - 	 Val. Loss: 0.505
2023-12-27 20:30 - INFO - 	 ROC-AUC: 0.711
2023-12-27 20:30 - INFO - 	 PR-AUC: 0.435
2023-12-27 20:30 - INFO - 	 Recall for 0.4 precision: 0.587
2023-12-27 20:30 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 20:30 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 20:30 - INFO - 	 Best PR-AUC: 0.450
2023-12-27 20:30 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 20:30 - INFO - ---------------------------------------------
2023-12-27 20:41 - INFO - ---------------------------------------------
2023-12-27 20:41 - INFO - Epoch: 11 | Time: 10m 55s
2023-12-27 20:41 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 20:41 - INFO - 	 Train Loss: 0.498
2023-12-27 20:41 - INFO - 	 Val. Loss: 0.499
2023-12-27 20:41 - INFO - 	 ROC-AUC: 0.723
2023-12-27 20:41 - INFO - 	 PR-AUC: 0.451
2023-12-27 20:41 - INFO - 	 Recall for 0.4 precision: 0.621
2023-12-27 20:41 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 20:41 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 20:41 - INFO - 	 Best PR-AUC: 0.451
2023-12-27 20:41 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 20:41 - INFO - ---------------------------------------------
2023-12-27 20:47 - INFO - ---------------------------------------------
2023-12-27 20:47 - INFO - Epoch: 12 | Time: 6m 28s
2023-12-27 20:47 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 20:47 - INFO - 	 Train Loss: 0.497
2023-12-27 20:47 - INFO - 	 Val. Loss: 0.510
2023-12-27 20:47 - INFO - 	 ROC-AUC: 0.698
2023-12-27 20:47 - INFO - 	 PR-AUC: 0.417
2023-12-27 20:47 - INFO - 	 Recall for 0.4 precision: 0.539
2023-12-27 20:47 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 20:47 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 20:47 - INFO - 	 Best PR-AUC: 0.451
2023-12-27 20:47 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 20:47 - INFO - ---------------------------------------------
2023-12-27 20:54 - INFO - ---------------------------------------------
2023-12-27 20:54 - INFO - Epoch: 13 | Time: 6m 34s
2023-12-27 20:54 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 20:54 - INFO - 	 Train Loss: 0.497
2023-12-27 20:54 - INFO - 	 Val. Loss: 0.507
2023-12-27 20:54 - INFO - 	 ROC-AUC: 0.720
2023-12-27 20:54 - INFO - 	 PR-AUC: 0.442
2023-12-27 20:54 - INFO - 	 Recall for 0.4 precision: 0.611
2023-12-27 20:54 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 20:54 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 20:54 - INFO - 	 Best PR-AUC: 0.451
2023-12-27 20:54 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 20:54 - INFO - ---------------------------------------------
2023-12-27 21:05 - INFO - ---------------------------------------------
2023-12-27 21:05 - INFO - Epoch: 14 | Time: 10m 54s
2023-12-27 21:05 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 21:05 - INFO - 	 Train Loss: 0.497
2023-12-27 21:05 - INFO - 	 Val. Loss: 0.512
2023-12-27 21:05 - INFO - 	 ROC-AUC: 0.701
2023-12-27 21:05 - INFO - 	 PR-AUC: 0.421
2023-12-27 21:05 - INFO - 	 Recall for 0.4 precision: 0.553
2023-12-27 21:05 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 21:05 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 21:05 - INFO - 	 Best PR-AUC: 0.451
2023-12-27 21:05 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 21:05 - INFO - ---------------------------------------------
2023-12-27 21:16 - INFO - ---------------------------------------------
2023-12-27 21:16 - INFO - Epoch: 15 | Time: 10m 43s
2023-12-27 21:16 - INFO - 	 New best val_rocauc loss was found, current best value is 0.72292
2023-12-27 21:16 - INFO - 	 Train Loss: 0.497
2023-12-27 21:16 - INFO - 	 Val. Loss: 0.527
2023-12-27 21:16 - INFO - 	 ROC-AUC: 0.707
2023-12-27 21:16 - INFO - 	 PR-AUC: 0.431
2023-12-27 21:16 - INFO - 	 Recall for 0.4 precision: 0.553
2023-12-27 21:16 - INFO - 	 Best Val. Loss: 0.496
2023-12-27 21:16 - INFO - 	 Best ROC-AUC: 0.723
2023-12-27 21:16 - INFO - 	 Best PR-AUC: 0.451
2023-12-27 21:16 - INFO - 	 Best Recall for 0.4 precision: 0.623
2023-12-27 21:16 - INFO - ---------------------------------------------
